{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if your data is not strictly related to fields commonly used in\n",
    "astrophysical codes or your code is not supported yet, you can still feed it to\n",
    "yt to use its advanced visualization and analysis facilities. The only\n",
    "requirement is that your data can be represented as three-dimensional NumPy arrays with a consistent grid structure. What follows are some common examples of loading in generic array data that you may find useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Unigrid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest case is that of a single grid of data spanning the domain, with one or more fields. The data could be generated from a variety of sources; we'll just give three common examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generated \"on-the-fly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common example is that of data that is generated in memory from the currently running script or notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll just create a 3-D array of random floating-point data using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = np.random.random(size=(64,64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load this data into yt, we need associate it with a field. The `data` dictionary consists of one or more fields, each consisting of a tuple of a NumPy array and a unit string. Then, we can call `load_uniform_grid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = dict(density = (arr, \"g/cm**3\"))\n",
    "bbox = np.array([[-1.5, 1.5], [-1.5, 1.5], [-1.5, 1.5]])\n",
    "ds = yt.load_uniform_grid(data, arr.shape, length_unit=\"Mpc\", bbox=bbox, nprocs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_uniform_grid` takes the following arguments and optional keywords:\n",
    "\n",
    "* `data` : This is a dict of numpy arrays, where the keys are the field names\n",
    "* `domain_dimensions` : The domain dimensions of the unigrid\n",
    "* `length_unit` : The unit that corresponds to `code_length`, can be a string, tuple, or floating-point number\n",
    "* `bbox` : Size of computational domain in units of `code_length`\n",
    "* `nprocs` : If greater than 1, will create this number of subarrays out of data\n",
    "* `sim_time` : The simulation time in seconds\n",
    "* `mass_unit` : The unit that corresponds to `code_mass`, can be a string, tuple, or floating-point number\n",
    "* `time_unit` : The unit that corresponds to `code_time`, can be a string, tuple, or floating-point number\n",
    "* `velocity_unit` : The unit that corresponds to `code_velocity`\n",
    "* `magnetic_unit` : The unit that corresponds to `code_magnetic`, i.e. the internal units used to represent magnetic field strengths.\n",
    "* `periodicity` : A tuple of booleans that determines whether the data will be treated as periodic along each axis\n",
    "\n",
    "This example creates a yt-native dataset `ds` that will treat your array as a\n",
    "density field in cubic domain of 3 Mpc edge size and simultaneously divide the \n",
    "domain into `nprocs` = 64 chunks, so that you can take advantage\n",
    "of the underlying parallelism. \n",
    "\n",
    "The optional unit keyword arguments allow for the default units of the dataset to be set. They can be:\n",
    "* A string, e.g. `length_unit=\"Mpc\"`\n",
    "* A tuple, e.g. `mass_unit=(1.0e14, \"Msun\")`\n",
    "* A floating-point value, e.g. `time_unit=3.1557e13`\n",
    "\n",
    "In the latter case, the unit is assumed to be cgs. \n",
    "\n",
    "The resulting `ds` functions exactly like a dataset like any other yt can handle--it can be sliced, and we can show the grid boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slc = yt.SlicePlot(ds, \"z\", [\"density\"])\n",
    "slc.set_cmap(\"density\", \"Blues\")\n",
    "slc.annotate_grids(cmap=None)\n",
    "slc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particle fields are detected as one-dimensional fields. Particle fields are then added as one-dimensional arrays in\n",
    "a similar manner as the three-dimensional grid fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posx_arr = np.random.uniform(low=-1.5, high=1.5, size=10000)\n",
    "posy_arr = np.random.uniform(low=-1.5, high=1.5, size=10000)\n",
    "posz_arr = np.random.uniform(low=-1.5, high=1.5, size=10000)\n",
    "data = dict(density = (np.random.random(size=(64,64,64)), \"Msun/kpc**3\"), \n",
    "            particle_position_x = (posx_arr, 'code_length'), \n",
    "            particle_position_y = (posy_arr, 'code_length'),\n",
    "            particle_position_z = (posz_arr, 'code_length'))\n",
    "bbox = np.array([[-1.5, 1.5], [-1.5, 1.5], [-1.5, 1.5]])\n",
    "ds = yt.load_uniform_grid(data, data[\"density\"][0].shape, length_unit=(1.0, \"Mpc\"), mass_unit=(1.0,\"Msun\"), \n",
    "                          bbox=bbox, nprocs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example only the particle position fields have been assigned. If no particle arrays are supplied, then the number of particles is assumed to be zero. Take a slice, and overlay particle positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slc = yt.SlicePlot(ds, \"z\", [\"density\"])\n",
    "slc.set_cmap(\"density\", \"Blues\")\n",
    "slc.annotate_particles(0.25, p_size=12.0, col=\"Red\")\n",
    "slc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 is a convenient format to store data. If you have unigrid data stored in an HDF5 file, it is possible to load it into memory and then use `load_uniform_grid` to get it into yt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import h5py\n",
    "from yt.config import ytcfg\n",
    "data_dir = ytcfg.get('yt','test_data_dir')\n",
    "from yt.utilities.physical_ratios import cm_per_kpc\n",
    "f = h5py.File(join(data_dir, \"UnigridData\", \"turb_vels.h5\"), \"r\") # Read-only access to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDF5 file handle's keys correspond to the datasets stored in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add some unit information. It may be stored in the file somewhere, or we may know it from another source. In this case, the units are simply cgs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "units = [\"gauss\",\"gauss\",\"gauss\", \"g/cm**3\", \"erg/cm**3\", \"K\", \n",
    "         \"cm/s\", \"cm/s\", \"cm/s\", \"cm/s\", \"cm/s\", \"cm/s\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over the items in the file handle and the units to get the data into a dictionary, which we will then load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {k:(v.value,u) for (k,v), u in zip(f.items(),units)}\n",
    "bbox = np.array([[-0.5, 0.5], [-0.5, 0.5], [-0.5, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = yt.load_uniform_grid(data, data[\"Density\"][0].shape, length_unit=250.*cm_per_kpc, bbox=bbox, nprocs=8, \n",
    "                       periodicity=(False,False,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the data came from a simulation which was 250 kpc on a side. An example projection of two fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prj = yt.ProjectionPlot(ds, \"z\", [\"z-velocity\",\"Temperature\",\"Bx\"], weight_field=\"Density\")\n",
    "prj.set_log(\"z-velocity\", False)\n",
    "prj.set_log(\"Bx\", False)\n",
    "prj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume Rendering Loaded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volume rendering requires defining a `TransferFunction` to map data to color and opacity and a `camera` to create a viewport and render the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the min and max of the field\n",
    "mi, ma = ds.all_data().quantities.extrema('Temperature')\n",
    "#Reduce the dynamic range\n",
    "mi = mi.value + 1.5e7\n",
    "ma = ma.value - 0.81e7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the properties and size of the `camera` viewport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose a vector representing the viewing direction.\n",
    "L = [0.5, 0.5, 0.5]\n",
    "# Define the center of the camera to be the domain center\n",
    "c = ds.domain_center[0]\n",
    "# Define the width of the image\n",
    "W = 1.5*ds.domain_width[0]\n",
    "# Define the number of pixels to render\n",
    "Npixels = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `camera` object and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = yt.create_scene(ds, 'Temperature')\n",
    "dd = ds.all_data()\n",
    "\n",
    "source = sc[0]\n",
    "\n",
    "source.log_field = False\n",
    "\n",
    "tf = yt.ColorTransferFunction((mi, ma), grey_opacity=False)\n",
    "tf.map_to_colormap(mi, ma, scale=15.0, colormap='algae')\n",
    "\n",
    "source.set_transfer_function(tf)\n",
    "\n",
    "sc.add_source(source)\n",
    "\n",
    "cam = sc.add_camera()\n",
    "cam.width = W\n",
    "cam.center = c\n",
    "cam.normal_vector = L\n",
    "cam.north_vector = [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.show(sigma_clip=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FITS image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FITS file format is a common astronomical format for 2-D images, but it can store three-dimensional data as well. The [AstroPy](https://www.astropy.org) project has modules for FITS reading and writing, which were incorporated from the [PyFITS](http://www.stsci.edu/institute/software_hardware/pyfits) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import astropy.io.fits as pyfits\n",
    "# Or, just import pyfits if that's what you have installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `pyfits` we can open a FITS file. If we call `info()` on the file handle, we can figure out some information about the file's contents. The file in this example has a primary HDU (header-data-unit) with no data, and three HDUs with 3-D data. In this case, the data consists of three velocity fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = pyfits.open(join(data_dir, \"UnigridData\", \"velocity_field_20.fits\"))\n",
    "f.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put it into a dictionary in the same way as before, but we slice the file handle `f` so that we don't use the `PrimaryHDU`. `hdu.name` is the field name and `hdu.data` is the actual data. Each of these velocity fields is in km/s. We can check that we got the correct fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for hdu in f:\n",
    "    name = hdu.name.lower()\n",
    "    data[name] = (hdu.data,\"km/s\")\n",
    "print (data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The velocity field names in this case are slightly different than the standard yt field names for velocity fields, so we will reassign the field names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"velocity_x\"] = data.pop(\"x-velocity\")\n",
    "data[\"velocity_y\"] = data.pop(\"y-velocity\")\n",
    "data[\"velocity_z\"] = data.pop(\"z-velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data into yt. Let's assume that the box size is a Mpc. Since these are velocity fields, we can overlay velocity vectors on slices, just as if we had loaded in data from a supported code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = yt.load_uniform_grid(data, data[\"velocity_x\"][0].shape, length_unit=(1.0,\"Mpc\"))\n",
    "slc = yt.SlicePlot(ds, \"x\", [\"velocity_x\",\"velocity_y\",\"velocity_z\"])\n",
    "for ax in \"xyz\":\n",
    "    slc.set_log(\"velocity_%s\" % (ax), False)\n",
    "slc.annotate_velocity()\n",
    "slc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic AMR Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion to unigrid data, data gridded into rectangular patches at varying levels of resolution may also be loaded into yt. In this case, a list of grid dictionaries should be provided, with the requisite information about each grid's properties. This example sets up two grids: a top-level grid (`level == 0`) covering the entire domain and a subgrid at `level == 1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_data = [\n",
    "    dict(left_edge = [0.0, 0.0, 0.0],\n",
    "         right_edge = [1.0, 1.0, 1.0],\n",
    "         level = 0,\n",
    "         dimensions = [32, 32, 32]), \n",
    "    dict(left_edge = [0.25, 0.25, 0.25],\n",
    "         right_edge = [0.75, 0.75, 0.75],\n",
    "         level = 1,\n",
    "         dimensions = [32, 32, 32])\n",
    "   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just fill each grid with random density data, with a scaling with the grid refinement level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for g in grid_data: \n",
    "    g[\"density\"] = (np.random.random(g[\"dimensions\"]) * 2**g[\"level\"], \"g/cm**3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particle fields are supported by adding 1-dimensional arrays to each `grid`. If a grid has no particles, the particle fields still have to be defined since they are defined elsewhere; set them to empty NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_data[0][\"particle_position_x\"] = (np.array([]), \"code_length\") # No particles, so set empty arrays\n",
    "grid_data[0][\"particle_position_y\"] = (np.array([]), \"code_length\")\n",
    "grid_data[0][\"particle_position_z\"] = (np.array([]), \"code_length\")\n",
    "grid_data[1][\"particle_position_x\"] = (np.random.uniform(low=0.25, high=0.75, size=1000), \"code_length\")\n",
    "grid_data[1][\"particle_position_y\"] = (np.random.uniform(low=0.25, high=0.75, size=1000), \"code_length\")\n",
    "grid_data[1][\"particle_position_z\"] = (np.random.uniform(low=0.25, high=0.75, size=1000), \"code_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, call `load_amr_grids`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = yt.load_amr_grids(grid_data, [32, 32, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_amr_grids` also takes the same keywords `bbox` and `sim_time` as `load_uniform_grid`. We could have also specified the length, time, velocity, and mass units in the same manner as before. Let's take a slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slc = yt.SlicePlot(ds, \"z\", [\"density\"])\n",
    "slc.annotate_particles(0.25, p_size=15.0, col=\"Pink\")\n",
    "slc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Particle Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both uniform grid data and AMR data, one can specify particle fields with multiple types if the particle field names are given as field tuples instead of strings (the default particle type is `\"io\"`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posxr_arr = np.random.uniform(low=-1.5, high=1.5, size=10000)\n",
    "posyr_arr = np.random.uniform(low=-1.5, high=1.5, size=10000)\n",
    "poszr_arr = np.random.uniform(low=-1.5, high=1.5, size=10000)\n",
    "posxb_arr = np.random.uniform(low=-1.5, high=1.5, size=20000)\n",
    "posyb_arr = np.random.uniform(low=-1.5, high=1.5, size=20000)\n",
    "poszb_arr = np.random.uniform(low=-1.5, high=1.5, size=20000)\n",
    "data = {\"density\": (np.random.random(size=(64,64,64)), \"Msun/kpc**3\"), \n",
    "        (\"red\", \"particle_position_x\"): (posxr_arr, 'code_length'), \n",
    "        (\"red\", \"particle_position_y\"): (posyr_arr, 'code_length'),\n",
    "        (\"red\", \"particle_position_z\"): (poszr_arr, 'code_length'),\n",
    "        (\"blue\", \"particle_position_x\"): (posxb_arr, 'code_length'), \n",
    "        (\"blue\", \"particle_position_y\"): (posyb_arr, 'code_length'),\n",
    "        (\"blue\", \"particle_position_z\"): (poszb_arr, 'code_length')}\n",
    "bbox = np.array([[-1.5, 1.5], [-1.5, 1.5], [-1.5, 1.5]])\n",
    "ds = yt.load_uniform_grid(data, data[\"density\"][0].shape, length_unit=(1.0, \"Mpc\"), mass_unit=(1.0,\"Msun\"), \n",
    "                          bbox=bbox, nprocs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see we have multiple particle types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = ds.all_data()\n",
    "print (ds.particle_types)\n",
    "print (dd[\"red\", \"particle_position_x\"].size)\n",
    "print (dd[\"blue\", \"particle_position_x\"].size)\n",
    "print (dd[\"all\", \"particle_position_x\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats for Loading Generic Array Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Particles may be difficult to integrate.\n",
    "* Data must already reside in memory before loading it in to yt, whether it is generated at runtime or loaded from disk. \n",
    "* Some functions may behave oddly, and parallelism will be disappointing or non-existent in most cases.\n",
    "* No consistency checks are performed on the hierarchy\n",
    "* Consistency between particle positions and grids is not checked; `load_amr_grids` assumes that particle positions associated with one grid are not bounded within another grid at a higher level, so this must be ensured by the user prior to loading the grid data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
